# 线程

- thread模块：用于小型任务，能力有限。
- threading模块：对线程提供了更强大、更高层次的支持。

## thread模块

thread.start_new_thread()方法接收以下参数：

- 需要运行的函数。
- 被运行函数的参数元组。
- 可选的命名参数字典。

示例：

```python
import thread
import time


# 打印时间5次，在每个“delay”之后
def print_time(threadName, delay):
    count = 0
    while count < 5:
        time.sleep(delay)
        count += 1
        print("%s: %s" % (threadName, time.ctime(time.time())))
        if delay == 2 and count == 2:
        thread.exit() # 正常终止线程


# 创建两个线程
try:
    thread.start_new_thread(print_time, ("Thread-1", 2))
    thread.start_new_thread(print_time, ("Thread-2", 4))
except as e:
    print(e)

# 保证程序一直运行
while True:
    pass
```

thread.allocate_lock方法返回锁供线程使用，而返回的锁对象有三个方法：

- acquire：为当前线程求求锁。可选的正整数作为参数。如果为0，
- release：为下一个线程释放锁。
- locked：如果锁已被某个线程请求则返回TRUE，反之FALSE。

```python
import thread
import time


global_value = 0

def run(threadName):
    global global_value
    print("%s with value %s" % (threadName, global_value))
    global_value += 1

# 多线程中锁对全局变量的处理
for i in range(10):
    thread.start_new_thread(run, ("Thread-" + str(i),))

while 1:
    pass
```

上面代码的输出结果为：

```shell
```

锁的请求和释放的例子：

```python
import thread
import time

global_value = 0
lock = thread.allocate_lock()

def run(threadName, lock):
    global global_value
    lock.acquire()
    local_copy = global_value
    print("%s with value %s" % (threadName, local_copy))
    global_value = local_copy + 1
    lock.release()

for i in range(10):
    thread.start_new_thread(run, ("Thread-" + str(=i), lock))

while 1:
    pass
```

## threading模块

应用场景比较：

- threading模块在 `I/O` 操作中能发挥最好，比如，下载远程资源、读取本地文件和目录。
- CPU多核计算应该使用multiprocessing模块

```python
import threading


def doubler(number):
    """
    """
    print(threading.currentThread().getName() + '\n')
    print(number * 2)
    print()


if __name__ == '__main__':
    for i in range(5):
        my_thread = threading.Thread(target=doubler, args=(i,))
        my_thread.start()
```

## GIL

全局解释器锁（Global Interpreter Lock）：由Python解释器控制的一次仅允许一个线程的锁。

变量引用计数示例：

```python
>>> import sys
>>> a = []
>>> b = a
>>> sys.getrefcount(a)
3
```

特征

- 一次仅允许一个线程处于执行状态。
- 对于计算密集型（CPU-bound）和多线程代码来说是性能瓶颈。

存在目的：

0x00) 内存中的使用引用计数变量追踪对象的引用次数，该变量的值为0时，对象占用的内存将会释放。

- 在竞争条件中，两个线程同时增加或者减少引用计数变量的值可以通过添加`锁`来保证安全。
- 对每个对象或者对象组添加锁会导致死锁（不止一个锁），以及重复获得和释放锁带来的性能下降。

### 计算密集型和读写密集型的比较

CPU-bound程序，对大限度的使用CPU，比如在矩阵乘法、搜索、图片处理等这类数学计算。

I/O-bound程序，花费时间在等待来自用户、文件、数据库、网络等的输入和输出。比如用户在输入提示时的思考、运行在数据库自身进程中的查询操作。

比较计算密集型计算中的单线程和多线程：

- GIL prevented the CPU-bound threads from executing in paralle；
- GIL does not have much impact on the performance of I/O-bound multi-threaded programs as the lock is shared between threads while they are waiting for I/O.

```python
# single_threaded.py
import time

COUNT = 50000000

def countdown(n):
    while n>0:
        n -= 1

start = time.time()
countdown(COUNT)
end = time.time()

print('Time taken in seconds -', end - start)
```

```python
# multi_threaded.py
import time
from threading import Thread

COUNT = 50000000

def countdown(n):
    while n>0:
        n -= 1

t1 = Thread(target=countdown, args=(COUNT//2,))
t2 = Thread(target=countdown, args=(COUNT//2,))

start = time.time()
t1.start()
t2.start()
t1.join()
t2.join()
end = time.time()

print('Time taken in seconds -', end - start)
```




## 总结

- GIL解决了Python的什么问题？

两个线程同时增加和减少引用计数变量的值，不释放所导致的内存泄露，不正确的释放（比如释放使用中对象引用的内存）导致的崩溃。

- 为什么选择GIL作为解决方案？
- 多线程在Python程序中的影响是什么？
- 为什么GIL还没被移除？attempts broke the existing C extensions which depend heavily on the solution that the GIL provides.  cannot bring a change as significant as the removal of GIL without causing backward incompatibility issues.
- 为什么在Python 3中没被移除？
- 如何使用Python GIL？

What is a thread?
How to define a thread
How to determine the current thread
How to use a thread in a subclass
Thread synchronization with a lock
Thread synchronization with an RLock
Thread synchronization with semaphores
Thread synchronization with a condition
Thread synchronization with an event
Thread synchronization with a barrier
Thread communication using a queue

####################################

How Python deals with threads
Unlike some other languages, Python uses multiple kernel-level threads that can each run
any of the interpreter-level threads. But the standard implementation of the
CPython language comes with a major limitation that renders threads less usable in many
contexts. All threads accessing Python objects are serialized by one global lock. This is done
because much of the interpreter internal structures, as well as third-party C code, are not
thread-safe and need to be protected.

This mechanism is called the Global Interpreter Lock (GIL), and its implementation details
on Python/C API level were already discussed in the Releasing GIL section of Chapter 9,
Python Extensions in Other Languages. The removal of GIL is a topic that occasionally
appears on the Python-dev emailing list and was postulated by developers multiple times.
Sadly, at the time of writing, no one has ever managed to provide a reasonable and simple
solution that would allow you to get rid of this limitation. It is highly improbable that we
will see any progress in this area anytime soon. It is safer to assume that GIL will stay in
CPython, and so we need to learn how to live with it.

So, what is the point of multithreading in Python?

那么，再Python中使用多线程的要点是什么？

When threads contain only pure Python code, there is little point in using threads to speed
up the program since the GIL will globally serialize the execution of all threads. But
remember that GIL cares only about Python code. In practice, the global interpreter lock is
released on a number of blocking system calls and can be released in sections of C
extensions that do not use any of Python/C API functions. This means that multiple threads
can do I/O operations or execute C code in certain third-party extensions in parallel.

当线程中之包含纯Python代码时，使用线程来加速程序时显得毫无意义，因为GIL在全局上会按照顺序来执行所有的线程。但你要记得，
GIL知观xin

Multithreading allows you to efficiently utilize time when a program is waiting for a thirdparty resource. This is because a sleeping thread that has explicitly released the GIL can
stand by and wake up when the results are back. Last, whenever a program needs to
provide a responsive interface, multithreading can be an answer, even in single-core
environments where the operating system needs to use time slicing. With multithreading,
the program can easily interact with the user while doing some heavy computing in the socalled background.

Note that GIL does not exist in every implementation of the Python language. It is a
limitation of CPython, Stackless Python, and PyPy, but does not exist in Jython and
IronPython (see Chapter 1, Current Status of Python). There has been some development of
the GIL-free version of PyPy, but at the time of writing this book, it is still at an
experimental stage and the documentation is lacking. It is based on
Software Transactional Memory and is called PyPy-STM. It is really hard to say when (or
if) it will be officially released as a production ready interpreter. Everything seems to
indicate that it won't happen soon.

## When should we use threading?
Despite the GIL limitation, threads can be really useful in some of the following cases:

尽管存在GIL的限制，线程仍旧能够使用在下面一些场景中：

Building responsive interfaces
Delegating work
Building multiuser applications

构建响应式界面
分发任务
构建多用户应用

Let's discuss the preceding cases in the next sections.

###　Building responsive interfaces
Let's say you ask your system to copy files from one folder to another through some
program with a graphical user interface. The task will possibly be pushed into the
background and the interface window will be constantly refreshed by the program. This
way, you get live feedback on the progress of the whole process. You will also be able to
cancel the operation. This is less irritating than a raw cp or copy shell command that does
not provide any feedback until the whole work is finished.

A responsive interface also allows a user to work on several tasks at the same time. For
instance, Gimp will let you play around with a picture while another one is being filtered,
since the two tasks are independent.

响应式界面允许用户同时操作多个任务。例如，Gimp

When trying to achieve such responsive interfaces, a good approach is to try to push longrunning tasks into the background, or at least try to provide constant feedback to the user.
The easiest way to achieve that is to use threads. In such a scenario, threads are not
intended to increase performance but only to make sure that the user can still operate the
interface, even if it needs to process some data for a longer period of time.
If such background tasks perform a lot of I/O operations, you are able to still get some
benefit from multicore CPUs. Then, it's a win-win situation.

当你尝试实现这类响应式接口时，其中一个好办法是你可以试着将长期运行任务放在后台，或者至少是把固定的信息反馈给用户。
最容易实现这个目标的使用线程。在此类场景中，线程不仅仅是提升性能，还能确保用户依旧可以操作界面，即便程序需要长时间的数据处理。

### Delegating work
If your application depends on many external resources, threads may really help in
speeding it up.

Let's consider the case of a function that indexes files in a folder and pushes the built
indexes into a database. Depending on the type of file, the function calls a different external
program. For example, one is specialized in PDFs and another one in OpenOffice files.

Delegating work
If your application depends on many external resources, threads may really help in
speeding it up.
Let's consider the case of a function that indexes files in a folder and pushes the built
indexes into a database. Depending on the type of file, the function calls a different external
program. For example, one is specialized in PDFs and another one in OpenOffice files.

Instead of processing all files in a sequence, by executing the right program and then
storing the results into the database, your function can set up a single thread for each
converter and push jobs to be done to each one of them through a queue. The overall time
taken by the function will be closer to the processing time of the slowest converter than to
the sum of all the work.

Note that such an approach is somewhat a hybrid between multithreading and
multiprocessing. If you delegate the work to external processes (for example, using the
run() function from the subprocess module), you are in fact doing work in multiple
processes, so this has symptoms of multiprocessing. Still, in our scenario, we are mainly
waiting for the processing of results being handled in separate threads, so it is still mostly
multithreading from the perspective of the Python code.
The other common use case for threads is performing multiple HTTP requests to an
external service. For instance, if you want to fetch multiple results from a remote web API,
it could take a lot of time to do that synchronously, especially if the remote server is located
in a distant location. If you wait for every previous response before making new requests,
you will spend a lot of time just waiting for the external service to respond, and additional
round-trip time delays will be added to every such request. If you are communicating with
some efficient service (Google Maps API, for instance), it is highly probable that it can serve
most of your requests concurrently without affecting the response times of separate
requests. It is then reasonable to perform multiple queries in separate threads. Remember
that when doing an HTTP request, the maximum time is spent on reading from the TCP
socket. This is a blocking I/O operation, so CPython will release the GIL when performing
the recv() C function. This allows for great improvement of your application's
performance

### Multiuser applications

Threading is also used as a concurrency base for multiuser applications. For instance, a web
server will push a user request into a new thread and then will become idle, waiting for
new requests. Having a thread dedicated to each request simplifies a lot of work, but
requires the developer to take care of locking the shared resources. But this is not a problem
when all the shared data is pushed into a relational database that takes care of the
concurrency matters. So, threads in a multiuser application act almost like separate
independent processes. They are under the same process only to ease their management at
the application level.

For instance, a web server will be able to put all requests in a queue and wait for a thread to
be available to send the work to it. Furthermore, it allows memory sharing that can boost
up some work and reduce the memory load. The two very popular Python WSGIcompliant webservers Gunicorn (refer to http://gunicorn.org/) and uWSGI (refer to
https:/​/​uwsgi-​docs.​readthedocs.​org) allow you to serve HTTP requests with threaded
workers in a way that generally follows this principle.

Using multithreading to enable concurrency in multiuser applications is less expensive than
using multiprocessing. Separate processes cost more resources since a new interpreter
needs to be loaded for each one of them. On the other hand, having too many threads is
expensive too. We know that GIL isn't such a problem for I/O extensive applications, but
there is always a time where you will need to execute Python code. Since you cannot
parallelize all of the application parts with bare threads, you will never be able to utilize all
of the resources on machines with multicore CPUs and a single Python process. This is
why, the optimal solution is often a hybrid of multiprocessing and
multithreading—multiple workers (processes) running with multiple threads. Fortunately,
many of the WSGI-compliant web servers allow such setup.

In the next section, we will take a look at an example of a threaded application.